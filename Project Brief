Project Brief for Ananta: Roam‑Style Note-Taking with Synchronized Audio Recording


Overview and Objectives
This project aims to create a Roam Research–style note-taking application enhanced with integrated audio recording. The application will allow a user to take hierarchically organized notes (bulleted outlines with indented sub-bullets), link between notes or specific blocks of text, and record audio from both microphone and system output simultaneously. The recorded audio is synchronized with the notes: as the user writes, the app timestamps each note “block” with the current audio time. A “play” button next to each block will play back the meeting/lecture audio starting from the moment that note was taken – similar to Microsoft OneNote’s audio notes feature
pressbooks.bccampus.ca
pressbooks.bccampus.ca
. The end goal is a clean, modern UI (inspired by OpenAI’s minimalistic design style) for a single-user desktop environment, backed by a PostgreSQL database for storing notes and audio. No user authentication is required (single-user, local use case). Key Features Summary:
Outliner Note Editor: Rich Markdown-compatible editor supporting bullet point hierarchies and sub-bullets (infinite nesting). Every bullet is a “block” that can contain nested child blocks
forum.zettelkasten.de
. Blocks can be referenced or linked elsewhere (each block will have a unique ID for linking, akin to Roam’s block references
forum.zettelkasten.de
).
Daily Notes Page: A special page for each calendar date, generated automatically each day (like a daily journal). The app should create or open the daily note page with the current date as title
nesslabs.com
, allowing the user to jot daily entries easily.
Linked References and Pages: Wiki-style linking of pages and blocks. For example, typing [[Page Name]] in a note creates a link to another page. Similarly, the user can insert a reference to any specific block (using a unique block ID or search UI, similar to Roam’s ((reference)) syntax) so that content can be transcluded or navigated to. Backlinks (seeing where a page or block is referenced) would be useful, though could be a later enhancement. Navigation should allow jumping to linked pages/blocks easily (possibly via sidebar or overlay panel).
Dual Audio Recording (Mic + System): Ability to record audio from the microphone and system output simultaneously (for example, to capture both the speaker’s voice and meeting audio). This is comparable to what OBS Studio does when recording, or OneNote during meetings. When recording starts, the app continuously captures both audio sources and mixes them into a single audio track (for simpler playback). Each audio recording session is associated with the note content being written.
Timestamped Blocks (Audio Sync): While recording is active, the editor notes the current audio timestamp each time a new block is created or an existing block is modified. Essentially, the app links the text to the moment in the audio it was written
pressbooks.bccampus.ca
. Later, when the user reviews notes, each block that has an associated timestamp will display a small “play” button. Clicking this button plays the recorded audio from that timestamp, so the user hears what was being said at the time they took that note
pressbooks.bccampus.ca
. This is exactly how OneNote’s synced audio notes feature works, and it greatly aids reviewing or filling gaps in notes.
Audio Playback per Block: For each block of text that has an associated audio timestamp, the UI shows a play control. On click, playback of the combined audio starts from that block’s timestamp. Basic controls (play/pause, maybe a way to seek a few seconds back or forward) should be available, but no waveform visualization or advanced audio editing is required – simple playback is sufficient.
Modern, Clean UI: The interface should be uncluttered and modern, drawing inspiration from OpenAI’s design aesthetic (lots of whitespace, clean typography, and subtle colors). For example, a lightweight dark-on-light theme, subtle highlights for links or tags, and simple icons for the play buttons and recording controls. The editing interface should feel like an outliner with smooth indent/outdent actions, similar to Roam or Workflowy. Keyboard shortcuts for navigation and editing (e.g. Tab to indent a bullet, Shift+Tab to outdent, Enter to create new bullet) should be supported for efficiency.
Backend and Storage: PostgreSQL will serve as the database. All pages, blocks, and relationships are stored in Postgres (e.g. pages and blocks tables in a graph or tree structure). The audio recordings are also stored in the database (likely as binary BLOB data in a BYTEA column). For a single-user with moderate audio usage, storing audio in Postgres is acceptable
stackoverflow.com
 and simplifies deployment (no separate file storage service). Each audio recording can be associated with a session or note (with metadata like duration, format). The text content itself can be stored as Markdown or as a structure of blocks (with parent/child references) for reconstructing the hierarchy.
Single-User, Local Setup: The app is intended for a single trusted user, so no authentication or user management is needed. It will run in a local environment (or single-user server) – essentially like a personal knowledge base. This simplifies development (no login/signup flows) and means we don’t need complex access control. We assume the environment is a desktop (for audio capture capabilities) and the user will run their own instance.
Feature Requirements in Detail
1. Core Note-Taking Features (Roam Research–style)
Outliner with Hierarchical Bullets: Users can create bullet points that nest to form a hierarchy (tree structure of thoughts). The editor should support indenting sub-bullets under parent bullets, and collapsing/expanding these lists. Each bullet is a “block” which can be treated as a unit of thought
forum.zettelkasten.de
. Blocks are identified uniquely (e.g. by an internal ID or UUID) so that they can be referenced elsewhere. The editor content can be stored as Markdown (with indentation representing hierarchy) or in a more structured form in the database.
Blocks and Block References: Every block should be linkable. The app will allow inserting a reference to an existing block. In Roam, every block has an ID and users can refer to that block, embedding its content or linking to it
forum.zettelkasten.de
. For MVP, we can implement a simple version: e.g., a context menu or Copy Reference action on a block that lets the user paste a special link (like ((block-id))) into another block, which on render shows the referenced block content or a link to jump to it. This allows the knowledge graph to form densely interconnected notes.
Wiki-Style Page Links and Tags: Users can create pages (top-level notes) and link to them from any block by using a special syntax (commonly [[Page Title]]). When the user types [[ the app could offer autocompletion of existing page titles. Clicking such a link navigates to that page. Similarly, a tag could be treated as a page link (often Roam treats #Tag as an alias for a page of that name
forum.zettelkasten.de
). We should support basic page linking so that users can easily jump between related notes.
Daily Notes Page: Provide a dedicated page that acts as a daily journal. Typically accessed via a “Daily Notes” button in the UI, it creates or opens today’s page. The page’s title is the current date (e.g. “2025-06-05”) and it’s basically a normal page for the user to jot daily thoughts, meeting notes, etc.
nesslabs.com
. This page is automatically created each day on first access and is a central hub for daily usage. (We might also include navigation for previous/next day).
Backlinks and Reference Navigation: When viewing a page, it’s useful to see where it’s mentioned. An MVP approach: list “linked references” at the bottom of a page – i.e. all other blocks or pages that link to the current page. This can be done with a simple query in the database for any [[CurrentPage]] mentions. For block references, possibly list where a block is referenced as well. This helps users navigate the graph of notes. While not explicitly asked, this is a common Roam feature and can be included if time permits (it might be phase 2 or nice-to-have in MVP).
Markdown Editor Functions: The editing interface should support basic Markdown formatting (bold, italic, inline code, maybe headings) either via keyboard shortcuts or syntax. At minimum, ensure that standard Markdown patterns (like **bold**, *italic*, `code`) are preserved in text or rendered accordingly. This can be done with a WYSIWYG approach or just plain text entry with live preview styling. Given the short timeline, a simple rich-text control or a Markdown editor component can be used. It should also handle pasting text, undo/redo, etc. A popular approach is to use an existing editor framework (like ProseMirror/TipTap, Slate.js, or even a simpler contenteditable with Markdown processing) to save time.
2. Audio Recording and Playback Features
Integrated Audio Recorder (Mic + System Sound): A core differentiator is the ability to record microphone and system audio simultaneously during note-taking. This means if the user is in a meeting (Zoom/Teams or in-person), the app captures both their microphone (personal comments or questions) and the system’s output (the meeting audio or any audio playing on the computer) at once. Achieving system audio capture is typically beyond standard browser capabilities due to sandboxing. For a seamless experience, we plan to implement this in a desktop application context (Electron or Tauri) where we can access native audio APIs. On Windows, for example, we can use the WASAPI loopback interface to capture system sound (or utilize a virtual audio driver if needed)
stackoverflow.com
. On macOS, capturing system audio usually requires installing a loopback driver (like Soundflower or Loopback) because macOS doesn’t allow it by default
stackoverflow.com
. On Linux, with PulseAudio or PipeWire, it’s possible to capture output via monitor devices. An open-source solution is to use the PortAudio library via a Node.js addon or similar, which provides cross-platform audio capture from input/output devices
github.com
github.com
. For instance, the Node package naudiodon wraps PortAudio and can record from designated input streams (mic or speaker output) and give us audio data in real-time
github.com
. We will mix the two audio sources into one track (either by the library if supported or manually mixing samples) so we have a single audio file with both sources combined.
Recording Controls in UI: The UI should have a clear way to start and stop recording. For example, a “Record” button (perhaps a red circle icon) that toggles recording mode. When activated, the app begins capturing audio and also indicates in the UI that recording is live (maybe a red dot icon and a timer). When the user stops recording, the audio data is finalized and saved to the database. Only one recording session is active at a time. We should consider where to place this control – possibly in a top toolbar or as part of the daily note page UI if that’s the primary use case. (We might also auto-start recording when opening a new daily page, but manual control is safer.)
Timestamping Notes: While recording is ongoing, every time the user creates a new block (or presses Enter to start a new bullet), the application should capture the current timestamp (offset in the audio stream) and attach it to that block’s data. Similarly, if the user is typing within a block for a long time, we might also tag the block with the start time it was created (since that’s when they likely heard something worth noting). If needed, we could also allow mid-block timestamps (for example, if one block is long and the user pauses typing, but this might be overkill). The primary idea is one timestamp per block at creation time. These timestamps are stored in the database along with the block (e.g. a field audio_timestamp which references seconds or milliseconds into the associated audio file). This design allows us to later retrieve “what was said when I wrote this block.”
Audio File Storage and Metadata: Each recording session produces an audio file (for MVP, we can use a standard format like WAV or MP3 – WAV for simplicity of writing, MP3 for smaller size if we include an encoder). This audio is saved as binary data in Postgres (using a BYTEA column). The database should have an AudioRecordings table, with fields like id, timestamp (when recording started), duration, maybe format, and the binary blob. Blocks that were recorded will have a foreign key reference to the audio record (if only one recording is expected at a time, we could store a single reference in a session table or in each block). Storing audio in the DB keeps everything self-contained. Given that usage is personal and within a few days scope, we anticipate the data volume is manageable
stackoverflow.com
. (If this were to scale to many hours of audio or many users, we might store files on disk or cloud and only keep links in DB, but not needed for MVP).
Playback Controls: In the note UI, any block that has an associated audio timestamp will display a small play button (for example, an icon to the left of the bullet, or on hover). The user can click play to hear the audio starting from that timestamp. Under the hood, the app will load the corresponding audio blob (likely by fetching from the local database or an API) and then seek to the timestamp and play. We can use a simple HTML5 <audio> element for playback or an audio API (even the Web Audio API) since once we have the audio file, playing it is straightforward. We do not need a waveform or complex scrubber UI; however, a basic progress bar or time display during playback might be nice. The key is that each block’s play button jumps the audio to the right spot
pressbooks.bccampus.ca
. If the user presses play on a different block, it should stop the current playback and start the new one. We should also consider indicating which block’s audio is playing (maybe highlight the block).
Recording Format and Quality: Use a reasonable audio format for recording. Possibly 44.1 kHz, 16-bit stereo (since we have two sources mixed). If using PortAudio or similar, we can capture raw PCM and encode to WAV. Alternatively, use Web Media APIs if in a browser context: e.g., the MediaRecorder API (but note: standard MediaRecorder can capture mic easily, capturing system audio would require using getDisplayMedia trick – not very user-friendly). For MVP in a desktop app, using Node+PortAudio or spawning an ffmpeg process could achieve capturing both sources. Open-source technique example: launching ffmpeg with appropriate input devices and an amix filter to combine mic and output into one stream. This could be a quick solution without writing low-level code: e.g., on Windows: ffmpeg -f dshow -i audio="virtual-audio-capturer":audio="Microphone (Realtek…)" -filter_complex amix -t <duration> output.wav. Since no budget, all tools must be open source or built-in – FFmpeg is free, PortAudio is free, etc. The development team can decide which approach is faster given their expertise (writing a Node native binding vs. calling an external process). The priority is to get functional dual-audio recording quickly.
Session Handling: We should define what happens if the user navigates away or stops writing while recording. Likely, the recording continues until manually stopped, regardless of what page the user is on. It might make sense to confine recording to the context of one page or session (for instance, you hit “Record” on the daily notes page and write notes there; if you navigate to another page, perhaps recording should either stop or the timestamp linking might get confusing). For MVP, a simple approach: maybe restrict recording to the Daily Note page or a single “current recording session” page. That way all timestamped blocks are on that one page. This simplifies implementation (no need to handle jumping between pages mid-record). In future, we could allow continuous recording across the app globally, but then we must handle linking audio times to blocks on various pages.
Transcription (Future Consideration): The brief doesn’t require transcription, but as an aside, some apps (OneNote, Noted app) also transcribe or allow searching in audio. This is out of scope for the 4-day MVP, but we note it as a potential extension: using something like Whisper AI to transcribe audio in the background to assist search. Noted (mentioned by a Logseq user
discuss.logseq.com
) and others do this, but we will focus on core recording and playback due to time.
3. Technology Stack and Architecture
Platform: Due to the need for system audio capture, a desktop application is the likely choice. We can create the app using Electron (JavaScript/Node.js + Chromium) or Tauri (Rust backend + webview). Both allow us to bundle a web-based UI with greater access to OS resources than a web browser. Electron is more mature with many Node modules available (e.g., PortAudio bindings, or ability to call ffmpeg, etc.), which might speed up development. Tauri is leaner (smaller footprint) and uses Rust for the backend, which could handle audio capture via crates like cpal (Rust audio library) or by invoking system APIs. Given the 4-day timeline, Electron with Node.js might be more feasible because we can leverage high-level libraries and JavaScript across the stack (faster iteration for a web developer). Electron also allows using browser APIs for parts of it and Node for native parts. We will wrap the app in one of these so that it’s essentially a local app that can bypass browser limitations.
Frontend Framework: To build a responsive, modern UI quickly, using a web framework like React (possibly with a UI toolkit or component library) is recommended. React is common and would integrate well in either Electron or Tauri. Other options like Vue or Svelte could also work if the team is faster with them. The UI will involve dynamic updating (especially the outliner, which is essentially a tree structure that the user modifies) and React’s state management or a library like Recoil or Redux could help manage the note graph state. However, for MVP, even a simpler state management or direct component state might suffice. If we choose Tauri+Rust, the frontend could still be React or even just plain HTML/JS, but we’ll assume a modern framework for speed.
Editor Component: Implementing a rich outliner from scratch is complex, so we should reuse open-source components if possible. One approach is to use ProseMirror (which can be configured for hierarchical content and has existing outliner examples) or TipTap (a developer-friendly wrapper around ProseMirror) to handle the editor view. TipTap, for instance, can be extended to support nested lists, custom commands for indent/outdent, and even custom node types for things like “block reference” nodes. Alternatively, Slate.js could be used for a more controlled rich text editing experience in React, allowing us to define each block as a Slate Node. Another lightweight possibility is using a Markdown text area and rendering it as an outline (like how some wiki editors do), but that might not give the smooth user experience of direct manipulation. Given time constraints, using a pre-built solution like TipTap with a list extension could save time, as it will manage the caret, formatting, etc.
Backend and Database: PostgreSQL will store the data. Since this is a single-user app, we might not need a complex backend server with REST APIs (the app could directly query the database if it’s running locally, or use an embedded database connection). However, to keep things organized, we might implement a lightweight API server (could be an Express.js app in Node or even use Electron’s IPC to handle DB queries). An alternative is using an ORM (Object-Relational Mapper) or query builder. For instance, Prisma or TypeORM can be used with Postgres to define Page, Block, and Audio entities and handle relations easily. This could accelerate development of the data layer. The schema might look like:
pages table: id, title, etc.
blocks table: id, page_id (nullable if block is top-level page?), parent_block_id (for hierarchy), content (text), audio_time (nullable, if recorded), audio_id (nullable, reference to AudioRecord if exists), created_at, etc.
audio_recordings table: id, created_at, data (BYTEA blob), duration, maybe format or sample rate.
Possibly a links table if we want to store page-to-page or block references explicitly (though those could also be inferred by parsing the content for [[ ]] or (( )) references).
For MVP, storing links implicitly (parsing content) is fine; we can implement explicit link storage later if needed for performance.
Audio Capture Libraries: As discussed, PortAudio via Node (naudiodon) is a prime candidate for capturing audio from both mic and speaker. This would run in the Electron main process (Node side). Another approach is to use the operating system’s facilities: for example, on Windows, one could use WASAPI directly. There are Node native addons or even C# libraries (if we considered a quick .NET helper) but that complicates the stack. Using FFmpeg as an external process is also possible if setting up PortAudio proves too time-consuming – simply spawn the process with correct device names (this requires the devices names which can be obtained via ffmpeg or PortAudio enumeration). Since the question specifically mentions OBS, one idea is to see if OBS’s code or plugins can be leveraged. OBS is open source (C++), but integrating it fully in 4 days is unrealistic. However, OBS has a virtual audio capture plugin on Windows (for per-app audio capture)
obsproject.com
 – not needed if we capture the entire system sound. We might simply note the technique: use a loopback audio source. There is also the possibility to use Web Audio API in Chrome to capture audio: Chrome’s navigator.mediaDevices.getDisplayMedia({audio:true}) can capture system audio when sharing the whole screen, and you could simultaneously get mic via getUserMedia and mix them in the browser
paul.kinlan.me
geeksforgeeks.org
. But that approach would prompt the user with a screen-sharing dialog and is not very seamless; it’s more of a hack. Thus, a direct desktop capture is preferred.
Frontend–Backend Integration: In an Electron context, the Renderer (front-end) can communicate with the Main process via IPC. So when the user clicks “Start Recording”, the renderer sends an IPC message to Main, which then starts the audio capture (using Node libs) and perhaps writes to a file or memory buffer. It can periodically send back updates (like current recording time if needed for a UI timer). When recording stops, the main process finalizes the file and either stores it in Postgres (via a DB connection on the main process side) or sends the blob to the renderer to store (probably easier to let main store it). For playing audio, the renderer could request the audio blob from the database (via an IPC call to main or direct DB access if we allow that in renderer, which we might not for security). The blob can then be loaded into an <audio> element as a Blob URL. All of this is doable within 4 days if we keep things straightforward and perhaps use synchronous approaches where acceptable (e.g., writing file then reading it to blob).
Open-Source Inspiration: We will draw UI and feature inspiration from existing open-source Roam-like projects:
Logseq: local-first outliner with pages and block references. It already supports linking, tags, and even has a request for audio recording with timestamps (similar to our goal)
discuss.logseq.com
discuss.logseq.com
. We can see how Logseq organizes its data (Logseq stores data in plain text Markdown/org files, but we can glean ideas for block structure and linking).
Athens Research: an open-source Roam clone (no longer maintained actively, but its approach of using a graph database and Datomic-like model could inform our schema)
news.ycombinator.com
.
Foam: a VSCode plugin for networked note-taking using Markdown files – we can learn from its Markdown link conventions and daily notes setup.
Org-roam: Roam for Org-mode Emacs – indicates how linking and daily notes can even be text-based; not directly applicable to our stack but conceptually similar.
TiddlyRoam / TiddlyBlink: experimental Roam features on top of TiddlyWiki – shows how a single-page app can handle a graph of notes and could inspire a lightweight approach.
We are not directly reusing code from these (different stacks and licenses), but they provide a reference for functionality and UI/UX decisions.
4. User Interface & Experience
Design Style: Emulate a clean, modern design like OpenAI’s interfaces. This means a focus on content: a primarily white (or lightly themed) background, simple typography (perhaps a font similar to OpenAI Sans or a neutral sans-serif). Use a minimal color palette – e.g., gentle highlight colors for bullets or links (OpenAI often uses shades of gray and one accent color like teal or purple for highlights). The UI should avoid heavy chrome or decorations. Think of the ChatGPT interface: mostly text on white, with subtle borders or shading for separation. We can incorporate slight shadows or rounded corners for panels to make it feel modern.
Layout: Likely a multi-panel layout:
A sidebar on the left for navigation (like Roam’s sidebar or graph overview). This could contain: a button for Daily Notes, a list of pages (or a search bar to find pages), and possibly later a graph view toggle. For MVP, we might include just a search box and daily notes button in the sidebar, and a list of all pages below that (sorted by recency or alphabetically).
The main content area on the right displays the currently open page (or the daily note). At the top of this area, show the page title (editable for normal pages, for daily notes it’s the date). Below that is the outliner of blocks.
If a block is a page reference or block reference, we might render it specially (e.g. as a small preview or a link that on click opens that page in the main view or in a sidebar).
We might also support opening pages side-by-side (Roam allows opening a second page in the right sidebar). For MVP we can skip multiple open pages to keep it simple.
Editing UX: When the user clicks on a bullet or presses Enter, a new bullet should appear. Indenting (Tab) and un-indenting (Shift+Tab) should reorganize the hierarchy. Drag-and-drop reordering of bullets is a nice-to-have; given the tight schedule, we might rely on keyboard for reordering (or simple cut-paste of content). Visual cues like an outliner guide (vertical line connecting sub-bullets) can help indicate hierarchy levels (as Roam and Logseq do).
Audio UI Integration: When recording is active, show an indicator – for example, a red dot in the top toolbar with a timer “Recording... 00:05:12”. The record button toggles to a stop button. Perhaps disable certain actions while recording if needed (though likely everything continues as normal). After stopping, perhaps show a “Recording saved” message.
Next to each block that has an audio timestamp, show a small play icon. The design of this could be a gray play triangle icon that appears to the left of the bullet (where list bullets normally are). OneNote shows a play icon on hover next to paragraphs that are linked to audio
pressbooks.bccampus.ca
. We can show it always or on hover. Clicking it should visually indicate playback (maybe the icon turns to a pause || icon while playing, or the block gets highlighted).
A global audio player control (e.g., at the bottom of the screen) could display the current playback progress when any block’s audio is playing. This could allow pausing or scrubbing the audio. However, given we only play from a start point per block and likely play until the user stops or plays something else, a minimal approach: clicking play on a block plays from there to the end of that audio file (or until user stops). We can include a simple bottom bar with a pause button and a progress bar if time permits, but it’s optional for MVP since the user can always click pause on the same play button (toggling it).
Responsive/Adaptability: Since this is a desktop app, we target typical laptop/desktop screen sizes. We don’t need a mobile design in MVP (no requirement for mobile, and capturing system audio on mobile would be impossible in a web app; if extended, it’d likely remain a desktop-focused tool). So we can focus on a design that works in a resizable window on desktop. Possibly allow the sidebar to collapse for more room.
Icons and Assets: Use open-source icon libraries (e.g., Feather icons or FontAwesome) for simple icons like play, stop, record, link, etc., to save time. Ensure they match the minimalist aesthetic (line icons might be good).
Accessibility Considerations: With limited time, we may not deeply address accessibility, but using semantic HTML where possible and allowing keyboard navigation in the editor is important for power users. The outline should be navigable with arrow keys as well (like up/down to move between blocks). This often comes for free if using a proper editor component.